{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scrape all tweets every 3 months!\n",
    "# 2. Store in csvs locally\n",
    "# 3. push csvs automatically into repo here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.graph_objects as go\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from dateutil import tz\n",
    "import glob\n",
    "import quandl as q\n",
    "import re\n",
    "import streamlit as st\n",
    "st.set_page_config(layout=\"wide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API credentials\n",
    "consumer_key = os.environ[\"twtr_consumer_key\"]\n",
    "consumer_secret = os.environ[\"twtr_consumer_secret\"]\n",
    "access_key = os.environ[\"twtr_access_key\"]\n",
    "access_secret = os.environ[\"twtr_access_secret\"]\n",
    "quandl_api_key = os.environ[\"quandl_api_key\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ThinkingUSD',\n",
       " 'BTCTN',\n",
       " 'Cov_duk',\n",
       " 'MFHoz',\n",
       " 'DrGreenApe',\n",
       " 'Ninjascalp',\n",
       " 'RaoulGMI',\n",
       " 'whale_alert',\n",
       " 'woonomic',\n",
       " 'thedefiedge',\n",
       " 'HsakaTrades',\n",
       " '100trillionUSD',\n",
       " 'depression2019',\n",
       " 'gametheorizing',\n",
       " 'krakenfx',\n",
       " 'saylor',\n",
       " 'eriz35',\n",
       " 'siliconebunker',\n",
       " 'BillyBobBaghold',\n",
       " 'crypto_color_',\n",
       " 'kamikaz_ETH',\n",
       " 'AviFelman',\n",
       " 'AltcoinGordon',\n",
       " 'lightcrypto',\n",
       " 'CryptoKaleo',\n",
       " 'APompliano',\n",
       " 'crypto_birb',\n",
       " 'iamDCinvestor',\n",
       " 'Grayscale',\n",
       " 'IamNomad',\n",
       " 'Cointelegraph',\n",
       " 'ErikVoorhees',\n",
       " 'AngeloBTC',\n",
       " 'TheCryptoDog',\n",
       " 'TommyThornton',\n",
       " 'hosseeb',\n",
       " 'AlamedaTrabucco',\n",
       " 'coinmamba',\n",
       " 'punk6529',\n",
       " 'cryptocobain',\n",
       " 'scottmelker',\n",
       " 'hasufl',\n",
       " 'john_j_brown',\n",
       " 'VitalikButerin',\n",
       " 'inmortalcrypto',\n",
       " 'hedgedhog7',\n",
       " 'Alice_comfy',\n",
       " 'cryptoPothu',\n",
       " 'LynAldenContact',\n",
       " 'BitcoinMagazine',\n",
       " 'santiagoroel',\n",
       " 'AriDavidPaul',\n",
       " 'ForbesCrypto',\n",
       " 'LifeNFT_',\n",
       " 'CryptoHayes',\n",
       " 'BarrySilbert',\n",
       " 'KanavKariya',\n",
       " 'Arthur_0x',\n",
       " 'wmd4x',\n",
       " 'cz_binance',\n",
       " 'Chubbicorn219']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_stored_twitter_user_csvs():\n",
    "    # get all csv file names - already scraped users\n",
    "    extension = 'csv'\n",
    "    all_twitter_user_scraped_csvs = glob.glob(\n",
    "        r'twitterdata/*.{}'.format(extension))  # CHANGE FOR SHARE STREAMLIT to /\n",
    "    # filter the price csv\n",
    "    all_twitter_user_scraped_csvs = [\n",
    "        k for k in all_twitter_user_scraped_csvs if 'BITFINEX' not in k]\n",
    "    # filter the price csv\n",
    "    all_twitter_user_scraped_csvs = [\n",
    "        k for k in all_twitter_user_scraped_csvs if 'relevant_words' not in k]\n",
    "    display_name_all_twitter_user_scraped_csvs = [\n",
    "        i.split(' ', 1)[0].split(\"twitterdata\\\\\")[1] for i in all_twitter_user_scraped_csvs]\n",
    "\n",
    "    return display_name_all_twitter_user_scraped_csvs, all_twitter_user_scraped_csvs\n",
    "\n",
    "display_name_all_twitter_user_scraped_csvs, all_twitter_user_scraped_csvs = get_all_stored_twitter_user_csvs()\n",
    "unique_display_name_all_twitter_user_scraped_csvs = list(set(display_name_all_twitter_user_scraped_csvs))\n",
    "unique_display_name_all_twitter_user_scraped_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_display_name_all_twitter_user_scraped_csvs = ['hodlKRYPTONITE' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hodlKRYPTONITE\n"
     ]
    }
   ],
   "source": [
    "for screen_name in unique_display_name_all_twitter_user_scraped_csvs:\n",
    "    print(screen_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hodlKRYPTONITE\n",
      "...400 tweets downloaded so far\n",
      "...600 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "for screen_name in unique_display_name_all_twitter_user_scraped_csvs:\n",
    "\n",
    "    try:\n",
    "        print(screen_name)\n",
    "        # Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "        # authorize twitter, initialize tweepy\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_key, access_secret)\n",
    "        api = tweepy.API(auth)\n",
    "\n",
    "        # initialize a list to hold all the tweepy Tweets\n",
    "        alltweets = []\n",
    "\n",
    "        try:\n",
    "            # make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "            new_tweets = api.user_timeline(screen_name=screen_name, count=200)\n",
    "        except:\n",
    "            st.error('Username does not exist')\n",
    "\n",
    "        # save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        # save the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        my_bar = st.progress(0)\n",
    "        progress_complete = 0\n",
    "\n",
    "        # keep grabbing tweets until there are no tweets left to grab\n",
    "        while len(new_tweets) > 0:\n",
    "\n",
    "            progress_complete += 7\n",
    "            if progress_complete >= 100:\n",
    "                progress_complete = 100\n",
    "            my_bar.progress(progress_complete)\n",
    "\n",
    "            # all subsiquent requests use the max_id param to prevent duplicates\n",
    "            new_tweets = api.user_timeline(\n",
    "                screen_name=screen_name, count=200, max_id=oldest)\n",
    "\n",
    "            # save most recent tweets\n",
    "            alltweets.extend(new_tweets)\n",
    "\n",
    "            # update the id of the oldest tweet less one\n",
    "            oldest = alltweets[-1].id - 1\n",
    "\n",
    "            print(f\"...{len(alltweets)} tweets downloaded so far\")\n",
    "\n",
    "        # transform the tweepy tweets into a 2D array that will populate the csv\n",
    "        outtweets = [[tweet.id_str, tweet.created_at, tweet.text]\n",
    "                        for tweet in alltweets]\n",
    "\n",
    "        # remove progress bar now after completion\n",
    "        my_bar.empty()\n",
    "        with open(r'twitterdata/{0} {1}.csv'.format(screen_name, oldest), 'w',  encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"id\", \"created_at\", \"text\"])\n",
    "            writer.writerows(outtweets)\n",
    "    except:\n",
    "        print(\"error for {}\".format(screen_name))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from git import Repo\n",
    "\n",
    "PATH_OF_GIT_REPO = r'C:\\Users\\gebel\\github\\\\streamlit_tweets_on_charts\\.git'\n",
    "now = datetime.datetime.now()\n",
    "COMMIT_MESSAGE = 'new tweet update {}'.format(now.date())\n",
    "\n",
    "repo = Repo(PATH_OF_GIT_REPO)\n",
    "repo.git.add(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<git.remote.PushInfo at 0x2c5dc176db0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.index.commit(COMMIT_MESSAGE)\n",
    "origin = repo.remote(name='origin')\n",
    "origin.push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('winning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71add3fad23a8c8e6b24a85f7630d2d04faae16b5347ce0e4a499f3d227a6bcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
